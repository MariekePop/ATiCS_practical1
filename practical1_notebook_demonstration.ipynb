{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fien/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import senteval\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download Glove embeddings\n",
    "Glove_model = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fully_connected_1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fully_connected_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fully_connected_3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.seq = nn.Sequential(\n",
    "            self.fully_connected_1,\n",
    "            self.fully_connected_2,\n",
    "            self.fully_connected_3,\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "def vocab(datapoint, vocabulary, UNK_TOKEN):\n",
    "    tokenised = datapoint.lower().split()\n",
    "    for token in tokenised:\n",
    "        if token in Glove_model:\n",
    "            vocabulary[token] = Glove_model[token]\n",
    "        else:\n",
    "            vocabulary[token] = vocabulary[UNK_TOKEN]\n",
    "    return vocab\n",
    "\n",
    "def evaluate_model(model, test_loader, checkpoint_path):\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for premise_hypothesis, label in test_loader:\n",
    "\n",
    "            output = model(premise_hypothesis)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            print(predicted)\n",
    "            correct = (predicted == label).sum().item()\n",
    "    try:\n",
    "        if predicted == 0:\n",
    "            model_guess = \"entailment\"\n",
    "        elif predicted == 1:\n",
    "            model_guess = \"neutral\"\n",
    "        elif predicted == 2:\n",
    "            model_guess = \"contradiction\" \n",
    "        \n",
    "        if label == 0:\n",
    "            correct_ans = \"entailment\"\n",
    "        elif label == 1:\n",
    "            correct_ans = \"neutral\"\n",
    "        elif label == 2:\n",
    "            correct_ans = \"contradiction\" \n",
    "    except:\n",
    "        print(\"Model could not make prediction, please inspect datapoint\")\n",
    "     \n",
    "    print(f\"The models answer was: {model_guess}, the correct label is: {correct_ans}\")\n",
    "    if correct == 1:\n",
    "        a = 1\n",
    "        print(\"So the model was correct\")\n",
    "    elif correct == 0:\n",
    "        print(\"So the model was wrong\")\n",
    "        # for nicer visual\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, embedded, lengths):\n",
    "        packed = rnn_utils.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, (hidden, _) = self.lstm(packed.float())\n",
    "        # Concatenate the last hidden states of forward and backward LSTMs\n",
    "        concatenated = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        return concatenated\n",
    "\n",
    "def get_embeddings_BiLSTM(datapoint, encoder, vocabulary):\n",
    "    lengths_list = []\n",
    "    tokenised = datapoint.lower().split()\n",
    "    indexed = [vocabulary[token] for token in tokenised if token in vocabulary and not np.all(vocabulary[token] == 0)]\n",
    "    if len(indexed) == 0:\n",
    "        return np.zeros(Glove_model.vector_size)\n",
    "    lengths_list.append(len(indexed))\n",
    "    indexed = torch.tensor(indexed).unsqueeze(0)\n",
    "    indexed = torch.nn.utils.rnn.pad_sequence(indexed, batch_first=True, padding_value=1)\n",
    "    embedding = encoder(indexed, torch.tensor(lengths_list))\n",
    "    return embedding.detach().numpy()\n",
    "\n",
    "def BiLSTM_dataset(data, lstm_encoder, vocabulary):\n",
    "    all_embeddings = {}\n",
    "    for datapoint in data:\n",
    "        premise_embedding = get_embeddings_BiLSTM(datapoint['premise'], lstm_encoder, vocabulary)\n",
    "        hypothesis_embedding = get_embeddings_BiLSTM(datapoint['hypothesis'], lstm_encoder, vocabulary)\n",
    "\n",
    "        # Check if premise_embedding and hypothesis_embedding have the same number of dimensions\n",
    "        if premise_embedding.shape[0] != hypothesis_embedding.shape[0]:\n",
    "            print(f\"Skipping datapoint: {datapoint}, premise_embedding and hypothesis_embedding have different dimensions\")\n",
    "            continue\n",
    "\n",
    "        concat_embeddings = torch.cat([torch.tensor(premise_embedding, dtype=torch.float32), torch.tensor(hypothesis_embedding, dtype=torch.float32)], dim=1)\n",
    "        elementwise_embeddings = torch.tensor(premise_embedding * hypothesis_embedding, dtype=torch.float32)\n",
    "        abs_diff_embeddings = torch.tensor(np.abs(premise_embedding - hypothesis_embedding), dtype=torch.float32)\n",
    "\n",
    "        embeddings = torch.cat([concat_embeddings, elementwise_embeddings, abs_diff_embeddings], dim=1).squeeze(0)\n",
    "        all_embeddings[torch.tensor(embeddings, dtype=torch.float32).clone().detach()] = torch.tensor(datapoint['label'], dtype=torch.long).clone().detach()\n",
    "    return all_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example(premise, hypothesis, label):\n",
    "    print(\"Premise:\", premise)\n",
    "    print(\"Hypothesis:\", hypothesis)\n",
    "    example_sentences = [{'premise': premise, \n",
    "                'hypothesis': hypothesis, \n",
    "                'label': label}]\n",
    "    \n",
    "    checkpoint_path = 'checkpoints_official/best_model_checkpoint_BiLSTM1.pth'\n",
    "\n",
    "    # add unseen word token\n",
    "    UNK_TOKEN = \"<UNK>\"\n",
    "    vocabulary = {UNK_TOKEN: np.random.rand(Glove_model.vector_size)}\n",
    "\n",
    "    for datapoint in example_sentences:\n",
    "        vocab(datapoint['premise'], vocabulary, UNK_TOKEN)\n",
    "        vocab(datapoint['hypothesis'], vocabulary, UNK_TOKEN)\n",
    "\n",
    "    # check longest datapoint for padding\n",
    "    longest = 0\n",
    "    for datapoint in example_sentences:\n",
    "        length = len(datapoint['premise'].lower().split())\n",
    "        if length > longest:\n",
    "            longest = length\n",
    "\n",
    "        length = len(datapoint['hypothesis'].lower().split())\n",
    "        if length > longest:\n",
    "            longest = length\n",
    "    # print(longest)\n",
    "\n",
    "    input_size = 300\n",
    "    hidden_size = 512\n",
    "    Bilstm_encoder = BiLSTMEncoder(input_size, hidden_size)\n",
    "\n",
    "    # BiLSTM version\n",
    "    BiLSTM_test_data = BiLSTM_dataset(example_sentences, Bilstm_encoder, vocabulary)\n",
    "\n",
    "    input_size = 4096\n",
    "    output_size = 3\n",
    "    BiLSTM_MLP_model = MLP(input_size, hidden_size, output_size)\n",
    "\n",
    "    # Convert baseline_train_data dictionary to a list of tuples and filter -1 labels\n",
    "    BiLSTM_test_data_list = [(embedding, label) for embedding, label in BiLSTM_test_data.items() if label != -1]\n",
    "\n",
    "    BiLSTM_test_loader = DataLoader(BiLSTM_test_data_list, batch_size=64)\n",
    "\n",
    "    evaluate_model(BiLSTM_MLP_model, BiLSTM_test_loader, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: A man is walking a dog\n",
      "Hypothesis: No cat is outside\n",
      "tensor([0])\n",
      "The models answer was: entailment, the correct label is: neutral\n",
      "So the model was wrong\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3888/2203168757.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_embeddings[torch.tensor(embeddings, dtype=torch.float32).clone().detach()] = torch.tensor(datapoint['label'], dtype=torch.long).clone().detach()\n"
     ]
    }
   ],
   "source": [
    "premise = \"A man is walking a dog\"\n",
    "hypothesis = \"No cat is outside\"\n",
    "label = 1\n",
    "\n",
    "example(premise, hypothesis, label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: Two men sitting in the sun\n",
      "Hypothesis: Nobody is sitting in the shade\n",
      "tensor([0])\n",
      "The models answer was: entailment, the correct label is: neutral\n",
      "So the model was wrong\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3888/2203168757.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_embeddings[torch.tensor(embeddings, dtype=torch.float32).clone().detach()] = torch.tensor(datapoint['label'], dtype=torch.long).clone().detach()\n"
     ]
    }
   ],
   "source": [
    "premise = \"Two men sitting in the sun\"\n",
    "hypothesis = \"Nobody is sitting in the shade\"\n",
    "label = 1\n",
    "\n",
    "example(premise, hypothesis, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important note\n",
    "Unfortunately our model(s) appear to consistently predict \"entailment\" regardless of the actual label. This means that our models perform no better than chance :(\n",
    "\n",
    "This behaviour can also be seen in the two examples above. We would expect the model to predict \"contradiction\" (eventhough the actual label is \"neutral\"), however we consiquently get the prediction \"entailment\". Despite this issue, we will continue to evaluate and discuss the models' performance in our report for the sake of completing the practical requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version that only prints the predicted value so it's easy to see the model only predicts \"entailment\"\n",
    "def evaluate_model2(model, test_loader, checkpoint_path):\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for premise_hypothesis, label in test_loader:\n",
    "\n",
    "            output = model(premise_hypothesis)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            print(predicted)\n",
    "\n",
    "\n",
    "def example2(premise, hypothesis, label):\n",
    "    example_sentences = [{'premise': premise, \n",
    "                'hypothesis': hypothesis, \n",
    "                'label': label}]\n",
    "    \n",
    "    checkpoint_path = 'checkpoints_official/best_model_checkpoint_BiLSTM1.pth'\n",
    "\n",
    "    # add unseen word token\n",
    "    UNK_TOKEN = \"<UNK>\"\n",
    "    vocabulary = {UNK_TOKEN: np.random.rand(Glove_model.vector_size)}\n",
    "\n",
    "    for datapoint in example_sentences:\n",
    "        vocab(datapoint['premise'], vocabulary, UNK_TOKEN)\n",
    "        vocab(datapoint['hypothesis'], vocabulary, UNK_TOKEN)\n",
    "\n",
    "    # check longest datapoint for padding\n",
    "    longest = 0\n",
    "    for datapoint in example_sentences:\n",
    "        length = len(datapoint['premise'].lower().split())\n",
    "        if length > longest:\n",
    "            longest = length\n",
    "\n",
    "        length = len(datapoint['hypothesis'].lower().split())\n",
    "        if length > longest:\n",
    "            longest = length\n",
    "    # print(longest)\n",
    "\n",
    "    input_size = 300\n",
    "    hidden_size = 512\n",
    "    Bilstm_encoder = BiLSTMEncoder(input_size, hidden_size)\n",
    "\n",
    "    # BiLSTM version\n",
    "    BiLSTM_test_data = BiLSTM_dataset(example_sentences, Bilstm_encoder, vocabulary)\n",
    "\n",
    "    input_size = 4096\n",
    "    output_size = 3\n",
    "    BiLSTM_MLP_model = MLP(input_size, hidden_size, output_size)\n",
    "\n",
    "    # Convert baseline_train_data dictionary to a list of tuples and filter -1 labels\n",
    "    BiLSTM_test_data_list = [(embedding, label) for embedding, label in BiLSTM_test_data.items() if label != -1]\n",
    "\n",
    "    BiLSTM_test_loader = DataLoader(BiLSTM_test_data_list, batch_size=64)\n",
    "\n",
    "    evaluate_model2(BiLSTM_MLP_model, BiLSTM_test_loader, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3888/2203168757.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_embeddings[torch.tensor(embeddings, dtype=torch.float32).clone().detach()] = torch.tensor(datapoint['label'], dtype=torch.long).clone().detach()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "# Here we see that for all datapoints in the testset, the model predicts \"entailment\"\n",
    "# even though this is wrong (for two thirds of the dataset)\n",
    "test_dataset = load_dataset('stanfordnlp/snli', split='test')\n",
    "# for time constrainst we show it for 100 datapoints\n",
    "test_dataset = test_dataset.select(range(100))\n",
    "for datapoint in test_dataset:\n",
    "    premise = datapoint['premise']\n",
    "    hypothesis = datapoint['hypothesis']\n",
    "    label = datapoint['label']\n",
    "    example2(premise, hypothesis, label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
